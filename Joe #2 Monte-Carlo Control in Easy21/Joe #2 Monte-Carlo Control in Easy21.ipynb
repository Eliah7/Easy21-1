{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from easy21 import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Apply Monte-Carlo control to Easy21. \n",
    "- [x] Initialise the value function to zero. \n",
    "- [x] Use a time-varying scalar step-size of αt = 1/N(st,at) \n",
    "- [x] and an ε-greedy exploration strategy with εt = N0/(N0 + N(st)), \n",
    "- [x] where N0 = 100 is a constant, \n",
    "- [x] N(s) is the number of times that state s has been visited, \n",
    "- [x] and N(s,a) is the number of times that action a has been selected from state s. \n",
    "- [x] Feel free to choose an alternative value for N0, if it helps producing better results. \n",
    "- [ ] Plot the optimal value function V ∗ (s) = maxa Q∗ (s, a) using similar axes to the following figure taken from Sutton and Barto’s Blackjack example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MC_Agent:\n",
    "    def __init__(self, environment, n0):\n",
    "        self.n0 = float(n0)\n",
    "        self.env = environment\n",
    "        \n",
    "        # N(s) is the number of times that state s has been visited\n",
    "        # N(s,a) is the number of times that action a has been selected from state s.\n",
    "        self.N = np.zeros((self.env.dealer_values_count,\n",
    "                           self.env.player_values_count, \n",
    "                           self.env.actions_count))\n",
    "        \n",
    "        self.Q = np.zeros((self.env.dealer_values_count,\n",
    "                           self.env.player_values_count, \n",
    "                           self.env.actions_count))\n",
    "        # self.E = np.zeros((self.env.dealer_values_count,\n",
    "        #                    self.env.player_values_count, \n",
    "        #                    self.env.actions_count))\n",
    "\n",
    "        # Initialise the value function to zero. \n",
    "        self.V = np.zeros((self.env.dealer_values_count, self.env.player_values_count))\n",
    "        \n",
    "        self.count_wins = 0\n",
    "        self.iterations = 0\n",
    "\n",
    "#     def get_action(self, s):\n",
    "#         a = Actions.hit\n",
    "#         return a\n",
    "    \n",
    "        # get optimal action, with epsilon exploration (epsilon dependent on number of visits to the state)\n",
    "    # ε-greedy exploration strategy with εt = N0/(N0 + N(st)), \n",
    "    def get_action(self, state):\n",
    "        dealer_idx = state.dealer-1\n",
    "        player_idx = state.player-1\n",
    "        n_visits = sum(self.N[dealer_idx, player_idx, :])\n",
    "\n",
    "        # epsilon = N0/(N0 + N(st)\n",
    "        curr_epsilon = self.n0 / (self.n0 + n_visits)\n",
    "\n",
    "        # epsilon greedy policy\n",
    "        if random.random() < curr_epsilon:\n",
    "            return Actions.hit if random.random()<0.5 else Actions.stick\n",
    "        else:\n",
    "            return Actions.to_action(np.argmax(self.Q[dealer_idx, player_idx, :]))\n",
    "\n",
    "    def train(self, iterations):        \n",
    "        \n",
    "        # Loop episodes\n",
    "        for episode in xrange(iterations):\n",
    "            episode_pairs = []\n",
    "            \n",
    "            # get initial state for current episode\n",
    "            s = self.env.get_start_state()\n",
    "            \n",
    "            # Execute until game ends\n",
    "            while not s.term:\n",
    "                \n",
    "                # get action with epsilon greedy policy\n",
    "                a = self.get_action(s)\n",
    "                \n",
    "                # store action state pairs\n",
    "                episode_pairs.append((s, a))\n",
    "                \n",
    "                # update visits\n",
    "                # N(s) is the number of times that state s has been visited\n",
    "                # N(s,a) is the number of times that action a has been selected from state s. \n",
    "                self.N[s.dealer-1, s.player-1, Actions.as_int(a)] += 1\n",
    "                \n",
    "                # execute action\n",
    "                s,r = self.env.step(s, a)\n",
    "\n",
    "            #if episode%10000==0: print \"Episode: %d, Reward: %d\" %(episode, my_state.rew)\n",
    "            self.count_wins = self.count_wins+1 if r==1 else self.count_wins\n",
    "\n",
    "            # Update Action value function accordingly\n",
    "            for curr_s, curr_a in episode_pairs:\n",
    "                # print s.dealer, s.player, s.r, a\n",
    "                dealer_idx = curr_s.dealer-1\n",
    "                player_idx = curr_s.player-1\n",
    "                action_idx = Actions.as_int(curr_a)\n",
    "                \n",
    "                # Use a time-varying scalar step-size of αt = 1/N(st,at) \n",
    "                step = 1.0 / sum(self.N[dealer_idx, player_idx, :])\n",
    "                error = r - self.Q[dealer_idx, player_idx, action_idx]\n",
    "                self.Q[dealer_idx, player_idx, action_idx] += step * error\n",
    "\n",
    "        self.iterations += iterations\n",
    "        print float(self.count_wins)/self.iterations*100\n",
    "\n",
    "        # Derive value function\n",
    "        for d in xrange(self.env.dealer_values_count):\n",
    "            for p in xrange(self.env.player_values_count):\n",
    "                self.V[d,p] = max(self.Q[d, p, :])\n",
    "\n",
    "# TODO \n",
    "#  add missing values\n",
    "#  make train so it can be ran multiple sets of itteractions\n",
    "#   (and figure out when to compute the value function and to make sure its done from start of file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# where N0 = 100 is a constant, \n",
    "N0 = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.018\n",
      "50.508\n",
      "50.9506666667\n",
      "51.1525\n",
      "51.3256\n",
      "51.4363333333\n",
      "51.5508571429\n",
      "51.62825\n",
      "51.6582222222\n",
      "51.7446\n"
     ]
    }
   ],
   "source": [
    "N0 = 100\n",
    "agent = MC_Agent(Environment(), N0)\n",
    "for i in xrange (10):\n",
    "    agent.train(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feel free to choose an alternative value for N0, if it helps producing better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48.242\n",
      "49.48\n",
      "49.9646666667\n",
      "50.3435\n",
      "50.5864\n",
      "50.7013333333\n",
      "50.8582857143\n",
      "50.964\n",
      "51.0493333333\n",
      "51.1312\n"
     ]
    }
   ],
   "source": [
    "N0 = 300\n",
    "agent = MC_Agent(Environment(), N0)\n",
    "for i in xrange (10):\n",
    "    agent.train(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.776\n",
      "50.677\n",
      "51.0226666667\n",
      "51.1155\n",
      "51.3028\n",
      "51.3463333333\n",
      "51.3622857143\n",
      "51.4335\n",
      "51.4482222222\n",
      "51.4692\n"
     ]
    }
   ],
   "source": [
    "N0 = 30\n",
    "agent = MC_Agent(Environment(), N0)\n",
    "for i in xrange (10):\n",
    "    agent.train(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.77\n",
      "50.492\n",
      "50.7446666667\n",
      "50.9765\n",
      "51.0676\n",
      "51.185\n",
      "51.2651428571\n",
      "51.38325\n",
      "51.4633333333\n",
      "51.5084\n"
     ]
    }
   ],
   "source": [
    "N0 = 110\n",
    "agent = MC_Agent(Environment(), N0)\n",
    "for i in xrange (10):\n",
    "    agent.train(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.658\n",
      "50.451\n",
      "50.978\n",
      "51.1715\n",
      "51.2976\n",
      "51.4263333333\n",
      "51.5165714286\n",
      "51.61475\n",
      "51.6937777778\n",
      "51.7638\n"
     ]
    }
   ],
   "source": [
    "N0 = 90\n",
    "agent = MC_Agent(Environment(), N0)\n",
    "for i in xrange (10):\n",
    "    agent.train(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}